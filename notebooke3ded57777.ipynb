{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11678375,"sourceType":"datasetVersion","datasetId":7329767}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/miranacareneandrisoa/notebooke3ded57777?scriptVersionId=238128470\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict,load_from_disk\nfrom transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, logging\nimport evaluate\nimport numpy as np\nimport os\nimport torch\nimport shutil\nimport re\nfrom typing import Optional\nimport types\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n# logging.set_verbosity_info() \ntorch.cuda.empty_cache()         # Releases unused cached memory\ntorch.cuda.ipc_collect()         # Collects inter-process communication memory\n\n\ntrain_src_path = \"../input/translation-model-en-mg/train_clean.en\"\ntrain_tgt_path = \"../input/translation-model-en-mg/train_clean.mg\"\nvalid_src_path = \"../input/translation-model-en-mg/valid_clean.en\"\nvalid_tgt_path = \"../input/translation-model-en-mg/valid_clean.mg\"\ndef delete_folder_if_exists(dir_path):\n    \"\"\"\n    Supprime le dossier spÃ©cifiÃ© si il existe.\n    \n    Args:\n        dir_path (str): Chemin vers le dossier Ã  supprimer.\n    \"\"\"\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)  # Supprime le dossier et son contenu\n        print(f\"Le dossier {dir_path} a Ã©tÃ© supprimÃ©.\")\n    else:\n        print(f\"Le dossier {dir_path} n'existe pas.\")\n# ðŸ”„ Fonction de chargement depuis fichiers alignÃ©s\n\n# delete_folder_if_exists(\"./cached_en_mg_tokenized\")\ndef load_translation_data(src_file, tgt_file, src_lang, tgt_lang):\n    with open(src_file, \"r\", encoding=\"utf-8\") as f:\n        src_lines = [line.strip() for line in f if line.strip()]\n    with open(tgt_file, \"r\", encoding=\"utf-8\") as f:\n        tgt_lines = [line.strip() for line in f if line.strip()]\n\n    print(f\"{src_file} contient {len(src_lines)} lignes non vides.\")\n    print(f\"{tgt_file} contient {len(tgt_lines)} lignes non vides.\")\n\n    assert len(src_lines) == len(tgt_lines), \"Les fichiers source et cible doivent avoir le mÃªme nombre de lignes.\"\n\n    return {\n        \"translation\": [\n            {src_lang: src, tgt_lang: tgt}\n            for src, tgt in zip(src_lines, tgt_lines)\n        ]\n    }\n\n\n# ðŸ“¦ CrÃ©ation des datasets\ntrain_data = load_translation_data(train_src_path, train_tgt_path, \"en\", \"mg\")\nvalid_data = load_translation_data(valid_src_path, valid_tgt_path, \"en\", \"mg\")\n\ndataset = DatasetDict({\n    \"train\": Dataset.from_dict(train_data),\n    \"validation\": Dataset.from_dict(valid_data)\n})\n\n\n\n\n# âš™ï¸ Chargement du tokenizer et modÃ¨le prÃ©-entraÃ®nÃ©\nmodel_name = \"Helsinki-NLP/opus-mt-en-mg\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"Cache GPU vidÃ©.\")\nelse:\n    print(\"CUDA n'est pas disponible. Aucun cache GPU Ã  vider.\")\n\nmodel = model.to(device)\n# âœ‚ï¸ PrÃ©traitement\ndef preprocess_function(examples):\n    inputs = [example['en'] for example in examples['translation']]\n    targets = [example['mg'] for example in examples['translation']]\n\n    model_inputs = tokenizer(\n        inputs, max_length=64, truncation=True, padding=\"max_length\", text_target=targets\n    )\n\n    return model_inputs\n\n\n# print(dataset['train'][0]) \n# exit()\n# ðŸ§  Tokenize the dataset\ncache_dir = \"cached_en_mg_tokenized\"\n\n# If the dataset is cached, load it from disk\ntry:\n    tokenized_dataset = load_from_disk(cache_dir)\n    print(\"Loaded tokenized dataset from cache.\")\nexcept:\n    # If not cached, process and save it\n    tokenized_dataset = dataset.map(\n        preprocess_function,\n        batched=True,\n        # num_proc=3,  # Use 3 CPU cores for faster mapping\n        load_from_cache_file=False  # Don't load from cache if it's being generated\n    )\n    # Save the tokenized dataset for future use\n    tokenized_dataset.save_to_disk(cache_dir)\n    print(f\"Tokenized dataset saved to {cache_dir}.\")\n\n# tokenized_dataset = dataset.map(preprocess_function, batched=True,num_proc=3)\n\n# ðŸ§  ParamÃ¨tres d'entraÃ®nement\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./en_to_mg_model\",\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",  # Sauvegarde Ã  chaque X pas\n    save_steps=4000,         # Sauvegarde toutes les 1000 Ã©tapes\n    eval_steps=4000,\n    learning_rate=2e-5,\n    per_device_train_batch_size=64,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    load_best_model_at_end=True,\n    resume_from_checkpoint=True,\n    fp16=True\n)\n\n# ðŸ” PrÃ©paration du Trainer\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\nmetric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n# Custom function to load RNG state correctly\ndef custom_load_rng_state(self, checkpoint):\n    checkpoint_dir = checkpoint\n    checkpoint_rng_state_file = os.path.join(checkpoint_dir, 'rng_state.pth')  # Adjust if your RNG state is saved under another name\n    if os.path.exists(checkpoint_rng_state_file):\n        # Load the full checkpoint without using weights_only\n        checkpoint_rng_state = torch.load(checkpoint_rng_state_file, weights_only=False)\n\n        # Ensure that the RNG state is only applied if the size matches\n        if \"cpu\" in checkpoint_rng_state:\n            torch.set_rng_state(checkpoint_rng_state[\"cpu\"])\n\n        # Ensure that CUDA RNG state is applied if CUDA is available\n        if torch.cuda.is_available():\n            if \"cuda\" in checkpoint_rng_state:\n                # Check if the CUDA state is of the correct size\n                if len(checkpoint_rng_state[\"cuda\"]) == torch.cuda.device_count():\n                    torch.cuda.set_rng_state_all(checkpoint_rng_state[\"cuda\"])\n                else:\n                    print(f\"Skipping CUDA RNG state loading: mismatched size.\")\n            else:\n                print(f\"Skipping CUDA RNG state loading: no 'cuda' state in checkpoint.\")\n        else:\n            print(f\"CUDA is not available, skipping CUDA RNG state loading.\")\n    else:\n        print(f\"RNG state file not found: {checkpoint_rng_state_file}\")\n\n# Replace the default load_rng_state method in the Trainer instance\ntrainer._load_rng_state = types.MethodType(custom_load_rng_state, trainer)\n\n\ndef get_last_checkpoint(checkpoint_root: str) -> Optional[str]:\n    \"\"\"\n    Returns the path to the latest checkpoint in the given directory,\n    or None if no valid checkpoint is found.\n    \"\"\"\n    if not os.path.isdir(checkpoint_root):\n        return None\n\n    checkpoints = [\n        d for d in os.listdir(checkpoint_root)\n        if re.match(r\"^checkpoint-\\d+$\", d)\n    ]\n\n    if not checkpoints:\n        return None\n\n    # Sort by checkpoint number\n    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n    return os.path.join(checkpoint_root, checkpoints[-1])\n\n# Utilisation\ncheckpoint_root = \"./en_to_mg_model\"\nlast_checkpoint = get_last_checkpoint(checkpoint_root)\n\n\n# ðŸš€ EntraÃ®nement\nif last_checkpoint:\n    print(f\"Reprise depuis le checkpoint : {last_checkpoint}\")\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    print(\"Aucun checkpoint trouvÃ©. DÃ©marrage depuis zÃ©ro.\")\n    trainer.train()\n\n# ðŸ’¾ Sauvegarde du modÃ¨le\ntrainer.save_model(\"./en_to_mg_model\")\ntokenizer.save_pretrained(\"./en_to_mg_model\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T09:48:21.793285Z","iopub.execute_input":"2025-05-06T09:48:21.794127Z"}},"outputs":[{"name":"stdout","text":"../input/translation-model-en-mg/train_clean.en contient 2101160 lignes non vides.\n../input/translation-model-en-mg/train_clean.mg contient 2101160 lignes non vides.\n../input/translation-model-en-mg/valid_clean.en contient 233463 lignes non vides.\n../input/translation-model-en-mg/valid_clean.mg contient 233463 lignes non vides.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cde60caa515441e9ec6a3e609fb7c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/796k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f38ce09151048c3a24a1e340aa9de59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/806k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b1760f01c84db784be31b4f8ed1350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01567b1e5504ffb8af9d54b827bcc3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da8933821319455d87dda29ad001b91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/291M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae2a67bcaec547d5acec33309f3d9ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ccc90a746764c23bb9405a3ab612e99"}},"metadata":{}},{"name":"stdout","text":"Cache GPU vidÃ©.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/291M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a55944ad0142fd89d7f185dc423b3a"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Loaded tokenized dataset from cache.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9212196cd49b4e15804ec205128297b3"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3357356111.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Reprise depuis le checkpoint : ./en_to_mg_model/checkpoint-8000\nSkipping CUDA RNG state loading: mismatched size.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12001' max='41040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12001/41040 2:00:57 < 14:38:21, 0.55 it/s, Epoch 1.46/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1680' max='3648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1680/3648 1:03:13 < 1:14:06, 0.44 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}